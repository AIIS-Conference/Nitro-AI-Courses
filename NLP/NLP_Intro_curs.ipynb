{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewBC62PLJWz",
        "outputId": "e2617d2a-6e43-4ab4-d331-de4fff98f508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "- Welcome to Nitro AI's workshop, hosted by AIIS conference on Natural Language Processing!\n",
            "- Natural Language Processing, or NLP, enables computers to understand human language.\n",
            "- Text preprocessing is a crucial step in NLP pipelines.\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    \"Welcome to Nitro AI's workshop, hosted by AIIS conference on Natural Language Processing!\",\n",
        "    \"Natural Language Processing, or NLP, enables computers to understand human language.\",\n",
        "    \"Text preprocessing is a crucial step in NLP pipelines.\",\n",
        "]\n",
        "\n",
        "print(\"Original Data:\")\n",
        "for text in data:\n",
        "    print(f\"- {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercase"
      ],
      "metadata": {
        "id": "k2DH_Wjk27vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lower = [text.lower() for text in data]\n",
        "print(\"\\nLowercase Data:\")\n",
        "for text in data_lower:\n",
        "  print(f\"- {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKWNWEER2_pD",
        "outputId": "de19d740-24d5-4d4b-ccc7-c8d1d5487297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lowercase Data:\n",
            "- welcome to nitro ai's workshop, hosted by aiis conference on natural language processing!\n",
            "- natural language processing, or nlp, enables computers to understand human language.\n",
            "- text preprocessing is a crucial step in nlp pipelines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "utAHIxm23SwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "data_tokens = [word_tokenize(text) for text in data_lower]\n",
        "print(\"\\nTokenized Data:\")\n",
        "for tokens in data_tokens:\n",
        "  print(f\"- {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePOy0gPC3VWD",
        "outputId": "5f748329-cfec-4f45-833b-b8a1671459c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Data:\n",
            "- ['welcome', 'to', 'nitro', 'ai', \"'s\", 'workshop', ',', 'hosted', 'by', 'aiis', 'conference', 'on', 'natural', 'language', 'processing', '!']\n",
            "- ['natural', 'language', 'processing', ',', 'or', 'nlp', ',', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n",
            "- ['text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'nlp', 'pipelines', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminarea punctuatiei"
      ],
      "metadata": {
        "id": "rDMWMz8r4ACw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_punctuation = [[word for word in tokens if word.isalnum()] for tokens in data_tokens]\n",
        "print(\"\\nData without Punctuation:\")\n",
        "for tokens in data_no_punctuation:\n",
        "  print(f\"- {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6ym9WWH4CEL",
        "outputId": "8f0168a7-b5b2-41c6-d081-441744b4ad79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data without Punctuation:\n",
            "- ['welcome', 'to', 'nitro', 'ai', 'workshop', 'hosted', 'by', 'aiis', 'conference', 'on', 'natural', 'language', 'processing']\n",
            "- ['natural', 'language', 'processing', 'or', 'nlp', 'enables', 'computers', 'to', 'understand', 'human', 'language']\n",
            "- ['text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'nlp', 'pipelines']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stopwords"
      ],
      "metadata": {
        "id": "dWnjqGdS4c5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data_no_stopwords = [[word for word in tokens if word not in stop_words] for tokens in data_no_punctuation]\n",
        "print(\"\\nData without Stopwords:\")\n",
        "for tokens in data_no_stopwords:\n",
        "  print(f\"- {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZN0PiFU4fBu",
        "outputId": "5b2e1c43-b0c1-49f0-fc71-56f56421a2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data without Stopwords:\n",
            "- ['welcome', 'nitro', 'ai', 'workshop', 'hosted', 'aiis', 'conference', 'natural', 'language', 'processing']\n",
            "- ['natural', 'language', 'processing', 'nlp', 'enables', 'computers', 'understand', 'human', 'language']\n",
            "- ['text', 'preprocessing', 'crucial', 'step', 'nlp', 'pipelines']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "mk_-5cwc5I63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "data_stemmed = [[ stemmer.stem(word) for word in tokens] for tokens in data_no_stopwords]\n",
        "print(\"\\nStemmed Data:\")\n",
        "for tokens in data_stemmed:\n",
        "  print(f\"- {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ4ZgNPk5KVh",
        "outputId": "4b3dee9f-9c79-427f-cf39-f9380896b057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed Data:\n",
            "- ['welcom', 'nitro', 'ai', 'workshop', 'host', 'aii', 'confer', 'natur', 'languag', 'process']\n",
            "- ['natur', 'languag', 'process', 'nlp', 'enabl', 'comput', 'understand', 'human', 'languag']\n",
            "- ['text', 'preprocess', 'crucial', 'step', 'nlp', 'pipelin']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "ke7JyjLW5skd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data_lemmatized = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in data_no_stopwords]\n",
        "print(\"\\nLemmatized Data:\")\n",
        "for tokens in data_lemmatized:\n",
        "  print(f\"- {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07oglD7-5vll",
        "outputId": "159ca7ff-ac17-483e-d288-7a64ce99845f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Data:\n",
            "- ['welcome', 'nitro', 'ai', 'workshop', 'hosted', 'aiis', 'conference', 'natural', 'language', 'processing']\n",
            "- ['natural', 'language', 'processing', 'nlp', 'enables', 'computer', 'understand', 'human', 'language']\n",
            "- ['text', 'preprocessing', 'crucial', 'step', 'nlp', 'pipeline']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaned = [\" \".join(tokens) for tokens in data_lemmatized]\n",
        "print(\"\\n Cleaned Data:\")\n",
        "for text in data_cleaned:\n",
        "  print(f\"- {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmHmMRUG6YNZ",
        "outputId": "3fb07022-4392-487c-8ea3-7690aea37453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Cleaned Data:\n",
            "- welcome nitro ai workshop hosted aiis conference natural language processing\n",
            "- natural language processing nlp enables computer understand human language\n",
            "- text preprocessing crucial step nlp pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "vocab = sorted(set(word for sentence in data_cleaned for word in sentence.split()))\n",
        "\n",
        "encoder= OneHotEncoder(sparse_output=False)\n",
        "encoded_vocab = encoder.fit_transform(np.array(vocab).reshape(-1,1))\n",
        "\n",
        "vocab_to_onehot = {word: encoded_vocab[i] for i, word in enumerate(vocab)}\n",
        "\n",
        "sentence_encodings = []\n",
        "for sentence  in data_cleaned:\n",
        "  encoding = [vocab_to_onehot[word] for word in sentence.split() if word in vocab]\n",
        "  sentence_encodings.append(encoding)\n",
        "\n",
        "print(\"Vocabular: \", vocab)\n",
        "print(\"\\nPropozitie originala:\", data_cleaned[0])\n",
        "print(\"\\nEncoding:\")\n",
        "for word, encoding in zip(data_cleaned[0].split(),sentence_encodings[0]):\n",
        "  print(f\"{word}: {encoding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrY5gDU88SM",
        "outputId": "f626d58d-fd09-45e3-8a28-9b23ffecc2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabular:  ['ai', 'aiis', 'computer', 'conference', 'crucial', 'enables', 'hosted', 'human', 'language', 'natural', 'nitro', 'nlp', 'pipeline', 'preprocessing', 'processing', 'step', 'text', 'understand', 'welcome', 'workshop']\n",
            "\n",
            "Propozitie originala: welcome nitro ai workshop hosted aiis conference natural language processing\n",
            "\n",
            "Encoding:\n",
            "welcome: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "nitro: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "ai: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "workshop: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "hosted: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "aiis: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "conference: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "natural: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "language: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "processing: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabular: \", vocab)\n",
        "print(\"\\nPropozitie originala:\", data_cleaned[1])\n",
        "print(\"\\nEncoding:\")\n",
        "for word, encoding in zip(data_cleaned[1].split(),sentence_encodings[1]):\n",
        "  print(f\"{word}: {encoding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcIRnJzR-kaG",
        "outputId": "b90ccded-2210-4704-e16a-155180cbabc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabular:  ['ai', 'aiis', 'computer', 'conference', 'crucial', 'enables', 'hosted', 'human', 'language', 'natural', 'nitro', 'nlp', 'pipeline', 'preprocessing', 'processing', 'step', 'text', 'understand', 'welcome', 'workshop']\n",
            "\n",
            "Propozitie originala: natural language processing nlp enables computer understand human language\n",
            "\n",
            "Encoding:\n",
            "natural: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "language: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "processing: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "nlp: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "enables: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "computer: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "understand: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "human: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "language: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words"
      ],
      "metadata": {
        "id": "efAVxkgk_Iqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer=CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(data_cleaned)"
      ],
      "metadata": {
        "id": "RLWSxMTT-8jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bow Matrix:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A8M52H2_Yzb",
        "outputId": "b3b5cc6b-3e1f-4f8a-92fe-8fab43090e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: ['ai' 'aiis' 'computer' 'conference' 'crucial' 'enables' 'hosted' 'human'\n",
            " 'language' 'natural' 'nitro' 'nlp' 'pipeline' 'preprocessing'\n",
            " 'processing' 'step' 'text' 'understand' 'welcome' 'workshop']\n",
            "Bow Matrix:\n",
            "[[1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1]\n",
            " [0 0 1 0 0 1 0 1 2 1 0 1 0 0 1 0 0 1 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y9uszP4_wXS",
        "outputId": "9d3e45ed-f206-4505-b923-e2934f03674f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ai  aiis  computer  conference  crucial  ...  step  text  understand  welcome  workshop\n",
            "0   1     1         0           1        0  ...     0     0           0        1         1\n",
            "1   0     0         1           0        0  ...     0     0           1        0         0\n",
            "2   0     0         0           0        1  ...     1     1           0        0         0\n",
            "\n",
            "[3 rows x 20 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data_cleaned)"
      ],
      "metadata": {
        "id": "Z2pdBumwAPaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiJMOFkfAjRC",
        "outputId": "5d125a26-5c3b-4965-8d4e-a91e2f332c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: ['ai' 'aiis' 'computer' 'conference' 'crucial' 'enables' 'hosted' 'human'\n",
            " 'language' 'natural' 'nitro' 'nlp' 'pipeline' 'preprocessing'\n",
            " 'processing' 'step' 'text' 'understand' 'welcome' 'workshop']\n",
            "[[0.338348   0.338348   0.         0.338348   0.         0.\n",
            "  0.338348   0.         0.25732238 0.25732238 0.338348   0.\n",
            "  0.         0.         0.25732238 0.         0.         0.\n",
            "  0.338348   0.338348  ]\n",
            " [0.         0.         0.35248004 0.         0.         0.35248004\n",
            "  0.         0.35248004 0.53614032 0.26807016 0.         0.26807016\n",
            "  0.         0.         0.26807016 0.         0.         0.35248004\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.42339448 0.\n",
            "  0.         0.         0.         0.         0.         0.32200242\n",
            "  0.42339448 0.42339448 0.         0.42339448 0.42339448 0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "print(similarity_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T1Rf76AA9AN",
        "outputId": "2083049f-4c4b-407f-cb75-6d77f108136a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.2759218  0.        ]\n",
            " [0.2759218  1.         0.08631924]\n",
            " [0.         0.08631924 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "for i, text in enumerate(data):\n",
        "  sentiment = TextBlob(text).sentiment\n",
        "  print(f\"Sentiment for '{text}': Polarity={sentiment.polarity}, Subjectivity={sentiment.subjectivity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVLrEsXABO8G",
        "outputId": "98666654-1c7b-4e67-dadf-d8f508cb67dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment for 'Welcome to Nitro AI's workshop, hosted by AIIS conference on Natural Language Processing!': Polarity=0.4625, Subjectivity=0.65\n",
            "Sentiment for 'Natural Language Processing, or NLP, enables computers to understand human language.': Polarity=0.05, Subjectivity=0.25\n",
            "Sentiment for 'Text preprocessing is a crucial step in NLP pipelines.': Polarity=0.0, Subjectivity=1.0\n"
          ]
        }
      ]
    }
  ]
}